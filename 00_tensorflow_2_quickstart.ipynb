{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 2 quicksart for beginners\n",
    "\n",
    "This introduction workbook uses `Keras` to:\n",
    "1. Load a prebuit dataset.\n",
    "2. Build a neural network machine learning model that classifies images.\n",
    "3. Train this neurl network.\n",
    "4. Evaluate the accuracy of the model.\n",
    "\n",
    "### Set up TensorFlaw\n",
    "\n",
    "Import TensorFlow into your program to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version:', tf.__version__) # tf.__version__ is a special attribute, providing version of a library thats installed in this environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a dataset\n",
    "\n",
    "Load and prepare the MNIST dataset. The pixel range of each images is 0 to 255. Scales from 0 to 1, by dividing them by `255.0`. This is so we work with them as floats, rather than in integers. Very important for later steps in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a machine learning model\n",
    "\n",
    "Build a `tf.keras.Sequential` model:\n",
    "\n",
    "* `Sequential` > Linearly stacked architectural layer of neural networks. Imagine a stack of pancakes, and you go through them from top to bottom, therefore the 'linear stack' term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\capma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)), # Converting 2d image input (28x28 image in this case), into 1d array. Typical first layer for image processing\n",
    "    tf.keras.layers.Dense(128, activation='relu'), # Fully connected layer with 128 neurons, and using the ReLU activation function.\n",
    "    tf.keras.layers.Dropout(0.2), # Regularization technique to prevent overfitting. Randomly setting a `rate` of input units to 0. rate > percentage of values\n",
    "    tf.keras.layers.Dense(10) # Another fully connected layer with 10 neurons. Often used for classification problems with 10 classes. (e.g. digits 0-9)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequentials are useful for stacking layers, where each layer accepts one input, and expells one output. Layers are functions with mathematical features/operations, that can be reused and have trainable variables. Most TensorFlow models are comprised of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02802001,  0.01520202,  0.170098  ,  0.17979825,  0.1942593 ,\n",
       "         0.08270757, -0.54096663,  0.0057817 , -0.13115457,  0.05920713]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy() # we're returning one row of prediction, just a small demo of model's performance\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seem like random numbers, but the model returns a vector of `logits` or `log-odds` scores, one for each class.\n",
    "\n",
    "So what are `logits` or `log-odds`?\n",
    "\n",
    "* `logits`: The output of the final, dense neural networks. The values provided are not probabilities, but raw scores that can be positive or negative.\n",
    "* `log-odds`: Aka Logarithmic odds. This is where you calculate the odds of something happening, compared to it not happening, but in logarithmic form. Where probabilities vary from 0 to 1, log-odd does it from -inf to inf respectively.\n",
    "\n",
    "The `tf.nn.softmax` function converts these logits to probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09536039, 0.09957243, 0.11625446, 0.11738764, 0.11909752,\n",
       "        0.10652619, 0.05709501, 0.09863883, 0.08601561, 0.10405196]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function for training using `losses.SparseCategoricalCrossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# SparseCategoricalCrossentropy > a loss function, commmonly used for multi-class classification problems, where target labels are integers\n",
    "# from_logits = True > Making sure the variables provided are logits, which are raw unormalized scores churned out from model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a vector of the truth values, and a vector of the logits, to return a scalar loss. This loss is equal to the negative log probability of the true class: For example, if loss is zero, then the model is sure the class is correct.\n",
    "\n",
    "This untrained model would give probability closer to random, (1 out of 10 for each class), so initial results is closer to - `tf.math.log(1/10)` ~= 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(2.2393644)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, configure and compile the model using Keras `Model.compile`. Set the `optimizer` class to `adam`, set the `loss` to the `loss_fn` function you deifned earlier, and specify a metric to be evaluated for the model by setting the `metrics` parameter to `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why are these parameters chosen? I've asked ChatGPT and get it's thought on the matter:\n",
    "\n",
    "`optimize='adam`: Adam (Adaptive Moment Estimation) uses two optimization algorithms: **Momentum**, and **RMSProp**. It performs well on a wide range of problems, and computationally efficient. Key features of it are adaptive learning rates which speeds up convergence, faster movement in firections with consistent gradients, and addresses biasness in early stages of training.\n",
    "\n",
    "`loss=loss_fn`: Using our loss function from before `SparseCategoricalCrossentropy(from_logits=True)`, suitable for multi-class classification problems. Key features is based on our dataset, where we use integer labels, and loss functions directly handle integer labels, making things efficient. `from_logits=True` is just making sure the output recieved is the raw scores from the model prediction.\n",
    "\n",
    "`metrics=['accuracy']`: Commonly used metric, to simply measure accuracy of our model's performance on this classification problem. Counting the number of corect predictions, from total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate your model\n",
    "\n",
    "Use the `model.fit` method to adjust your model parameters and minimize the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.8602 - loss: 0.4844\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.1528\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.1066\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.0882\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9788 - loss: 0.0729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x272dd4a2190>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5) # epochs > the number of times we train on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model.evaluate` method checks the model's performace, usually on a `validation set` or `test set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 5ms/step - accuracy: 0.9763 - loss: 0.0761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07607124000787735, 0.9763000011444092]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image classifier is now trained to ~98% accuracy to this dataset. If you want your model to return a probability, you can wrap the trained model, and atach the softmax to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([ # used to stack the original model, with the softmax layers, and create a work progress pipeline\n",
    "    model, # outputting raw logits of the prediction data\n",
    "    tf.keras.layers.Softmax() # for converting logits to probabilities - all values between 0 and 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.95702705e-06, 1.04122485e-07, 2.57920528e-05, 4.18161129e-04,\n",
       "        4.18612783e-11, 4.89322531e-07, 3.83468973e-13, 9.99530792e-01,\n",
       "        1.36535630e-06, 2.14940337e-05],\n",
       "       [1.66741502e-05, 1.48170147e-04, 9.99769151e-01, 1.56249753e-05,\n",
       "        4.03221068e-17, 2.28980953e-05, 2.48716970e-05, 1.34645713e-13,\n",
       "        2.65755421e-06, 3.98775061e-13],\n",
       "       [1.97221766e-06, 9.98739660e-01, 1.52511959e-04, 3.75516424e-06,\n",
       "        6.16409743e-05, 2.21513183e-05, 1.77154026e-04, 5.48244861e-04,\n",
       "        2.92281009e-04, 7.41802523e-07],\n",
       "       [9.99806464e-01, 4.02196143e-09, 1.64343310e-05, 3.79205067e-09,\n",
       "        1.00194723e-06, 4.74255984e-07, 1.75137451e-04, 1.40318139e-08,\n",
       "        3.21859073e-09, 5.92901984e-07],\n",
       "       [2.35124462e-05, 6.74561761e-07, 1.25323970e-06, 1.17560596e-07,\n",
       "        9.97681618e-01, 3.85828230e-07, 5.37731376e-06, 1.37466312e-04,\n",
       "        1.04605874e-06, 2.14859657e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the probabilities for every list of values. See how tiny the values are, showing how the model is certain that they are not the right label to the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congrats! I have trained a machine learning model using a prebuilt dataset using the `Keras` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
